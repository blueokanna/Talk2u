use super::cognitive_engine::CognitiveEngine;
use super::conversation_store::ConversationStore;
use super::data_models::*;
use super::error_handler::ChatError;
use super::jwt_auth::JwtAuth;
use super::memory_engine::MemoryEngine;
use super::saydo_detector::SayDoDetector;
use super::streaming_handler::StreamingHandler;

const BIGMODEL_API_URL: &str = "https://open.bigmodel.cn/api/paas/v4/chat/completions";

pub struct ChatEngine {
    jwt_auth: std::sync::Mutex<JwtAuth>,
    conversation_store: ConversationStore,
    memory_engine: MemoryEngine,
}

impl ChatEngine {
    pub fn new(api_key: &str, data_path: &str) -> Result<Self, String> {
        let jwt_auth = JwtAuth::new(api_key)?;
        let conversation_store = ConversationStore::new(data_path);
        let memory_engine = MemoryEngine::new(data_path);
        Ok(Self {
            jwt_auth: std::sync::Mutex::new(jwt_auth),
            conversation_store,
            memory_engine,
        })
    }

    /// Validate message content â€” reject blank messages (whitespace-only).
    pub fn validate_message(content: &str) -> Result<(), ChatError> {
        if content.trim().is_empty() {
            return Err(ChatError::ValidationError {
                message: "Message cannot be blank".to_string(),
            });
        }
        Ok(())
    }

    /// è‡ªåŠ¨æ£€æµ‹æ¶ˆæ¯çš„ say/do ç±»å‹
    pub fn detect_message_type(content: &str) -> MessageType {
        SayDoDetector::detect(content)
    }

    /// æ ¹æ®æ¨¡å‹è‡ªåŠ¨å†³å®šæ˜¯å¦å¯ç”¨æ€è€ƒ
    /// glm-4-air â†’ è‡ªåŠ¨å¼€å¯æ€è€ƒ
    /// glm-4.7 / glm-4.7-flash â†’ ä¸æ”¯æŒæ€è€ƒ
    pub fn should_enable_thinking(model: &str, user_preference: bool) -> bool {
        match model {
            // glm-4-air: ç”¨æˆ·å¯é€‰
            "glm-4-air" => user_preference,
            // glm-4.7: ç”¨æˆ·å¯é€‰ï¼ˆAPI é»˜è®¤å¼€å¯ï¼Œéœ€è¦æ˜¾å¼æ§åˆ¶ï¼‰
            "glm-4.7" => user_preference,
            // flash æ¨¡å‹é»˜è®¤ä¸å¼€å¯æ€è€ƒï¼ŒèŠ‚çœ token
            "glm-4.7-flash" => false,
            _ => false,
        }
    }

    /// ä¼°ç®—æ¶ˆæ¯åˆ—è¡¨çš„ token æ•°ï¼ˆç²—ç•¥ï¼šä¸­æ–‡1å­—â‰ˆ1.5tokenï¼Œè‹±æ–‡1è¯â‰ˆ1tokenï¼‰
    pub fn estimate_token_count(messages: &[Message]) -> usize {
        let mut total_chars: usize = 0;
        for msg in messages {
            total_chars += msg.content.len();
        }
        // ç²—ç•¥ä¼°ç®—ï¼šUTF-8 å­—èŠ‚æ•° / 2 â‰ˆ token æ•°ï¼ˆä¸­è‹±æ··åˆåœºæ™¯çš„åˆç†è¿‘ä¼¼ï¼‰
        total_chars / 2
    }

    /// æ ¹æ®ä¸Šä¸‹æ–‡é•¿åº¦é€‰æ‹©æ€»ç»“æ¨¡å‹
    /// è¶…è¿‡ 128K token ä½¿ç”¨ glm-4-longï¼Œå¦åˆ™ä½¿ç”¨ glm-4.7-flash
    pub fn choose_summary_model(messages: &[Message]) -> &'static str {
        let estimated_tokens = Self::estimate_token_count(messages);
        if estimated_tokens > 128_000 {
            "glm-4-long"
        } else {
            "glm-4.7-flash"
        }
    }

    /// Build the BigModel API request body.
    pub fn build_request_body(
        messages: &[Message],
        model: &str,
        enable_thinking: bool,
    ) -> serde_json::Value {
        Self::build_request_body_with_options(messages, model, enable_thinking, None)
    }

    /// Build the BigModel API request body with optional max_tokens override.
    pub fn build_request_body_with_options(
        messages: &[Message],
        model: &str,
        enable_thinking: bool,
        max_tokens_override: Option<u32>,
    ) -> serde_json::Value {
        let api_messages: Vec<serde_json::Value> = messages
            .iter()
            .map(|m| {
                let role = match m.role {
                    MessageRole::User => "user",
                    MessageRole::Assistant => "assistant",
                    MessageRole::System => "system",
                };
                serde_json::json!({
                    "role": role,
                    "content": m.content,
                })
            })
            .collect();

        let mut body = serde_json::json!({
            "model": model,
            "messages": api_messages,
            "stream": true,
        });

        // è®¾ç½® max_tokens
        if let Some(max_tokens) = max_tokens_override {
            body["max_tokens"] = serde_json::json!(max_tokens);
        } else {
            match model {
                "glm-4.7" | "glm-4.7-flash" => {
                    if enable_thinking {
                        // æ€è€ƒæ¨¡å¼ä¸‹ max_tokens åŒ…å« reasoning + contentï¼Œéœ€è¦è¶³å¤Ÿå¤§
                        body["max_tokens"] = serde_json::json!(4096);
                    } else {
                        // éæ€è€ƒæ¨¡å¼ï¼Œè§’è‰²æ‰®æ¼”å¯¹è¯é€šå¸¸ä¸éœ€è¦å¤ªé•¿
                        body["max_tokens"] = serde_json::json!(1024);
                    }
                }
                "glm-4-air" => {
                    body["max_tokens"] = serde_json::json!(4096);
                }
                _ => {} // glm-4-long ç­‰æ€»ç»“æ¨¡å‹ä¸é™åˆ¶
            }
        }

        // æ™ºè°± API é»˜è®¤å¼€å¯ thinkingï¼Œå¿…é¡»æ˜¾å¼æ§åˆ¶
        // GLM-4.7/GLM-4.7-flash/GLM-4-air ç­‰æ¨¡å‹éƒ½æ”¯æŒ thinking å‚æ•°
        match model {
            "glm-4.7" | "glm-4.7-flash" | "glm-4-air" => {
                if enable_thinking {
                    body["thinking"] = serde_json::json!({"type": "enabled"});
                } else {
                    body["thinking"] = serde_json::json!({"type": "disabled"});
                }
            }
            _ => {}
        }

        body
    }

    /// æ„å»ºå¸¦è®°å¿†ä¸Šä¸‹æ–‡å¢å¼ºçš„æ¶ˆæ¯åˆ—è¡¨
    /// å®ç°è‡ªæˆ‘è®¤çŸ¥æ¶æ„ï¼š
    ///   å±‚1: è§’è‰²èº«ä»½é”šå®šï¼ˆsystem promptï¼‰
    ///   å±‚2: è®°å¿†ä¸Šä¸‹æ–‡æ³¨å…¥ï¼ˆå†å²è®°å¿†æ£€ç´¢ç»“æœï¼‰
    ///   å±‚3: æƒ…æ„ŸçŠ¶æ€è¿½è¸ªï¼ˆåŸºäºæœ€è¿‘å¯¹è¯æ¨æ–­å½“å‰æƒ…ç»ªåŸºçº¿ï¼‰
    ///   å±‚4: å¯¹è¯å†å²çª—å£ï¼ˆæœ€è¿‘ 20 æ¡æ¶ˆæ¯ï¼‰
    ///   å±‚5: é£æ ¼çº¦æŸï¼ˆsay/do æ¨¡å¼æç¤ºï¼‰
    pub fn build_context_enhanced_messages(
        conv: &Conversation,
        user_content: &str,
        memory_summaries: &[MemorySummary],
    ) -> Vec<Message> {
        let mut enhanced_messages: Vec<Message> = Vec::new();

        // å±‚1: ä¿ç•™è§’è‰² system æ¶ˆæ¯ï¼ˆèº«ä»½é”šå®šï¼‰
        let mut system_token_budget: usize = 0;
        for msg in &conv.messages {
            if msg.role == MessageRole::System {
                enhanced_messages.push(msg.clone());
                system_token_budget += msg.content.len() / 2;
                break;
            }
        }

        // å±‚2: æ£€ç´¢ç›¸å…³è®°å¿†å¹¶æ³¨å…¥ä¸Šä¸‹æ–‡
        if !memory_summaries.is_empty() {
            let search_results =
                MemoryEngine::search_memories(user_content, memory_summaries, 3);

            if !search_results.is_empty() {
                let mut context = String::from("ã€å†å²è®°å¿†ä¸Šä¸‹æ–‡ã€‘\n");
                for result in &search_results {
                    context.push_str(&format!("- {}\n", result.summary));
                    for fact in &result.core_facts {
                        context.push_str(&format!("  æ ¸å¿ƒäº‹å®ï¼š{}\n", fact));
                    }
                }
                context.push_str("åŸºäºä»¥ä¸Šè®°å¿†ä¿æŒè§’è‰²ä¸€è‡´æ€§ã€‚\n");

                system_token_budget += context.len() / 2;
                enhanced_messages.push(Message {
                    id: String::new(),
                    role: MessageRole::System,
                    content: context,
                    thinking_content: None,
                    model: "system".to_string(),
                    timestamp: 0,
                    message_type: MessageType::Say,
                });
            }
        }

        // å±‚3: è®¤çŸ¥æ€ç»´å¼•æ“ï¼ˆæ›¿ä»£ç®€å•çš„æƒ…æ„Ÿå…³é”®è¯åŒ¹é…å’Œè¿è´¯æ€§æ£€æµ‹ï¼‰
        // æ•´åˆäº†ï¼šæƒ…æ„Ÿæ„ŸçŸ¥ã€è¯­è¨€æ¨¡å¼æ£€æµ‹ã€æ„å›¾æ¨æ–­ã€å…³ç³»åˆ†æã€å…±æƒ…ç­–ç•¥
        let non_system: Vec<&Message> = conv
            .messages
            .iter()
            .filter(|m| m.role != MessageRole::System)
            .collect();

        if non_system.len() >= 2 {
            let cognitive_analysis = CognitiveEngine::analyze(&non_system);
            let cognitive_prompt = cognitive_analysis.cognitive_prompt;
            if !cognitive_prompt.is_empty() {
                system_token_budget += cognitive_prompt.len() / 2;
                enhanced_messages.push(Message {
                    id: String::new(),
                    role: MessageRole::System,
                    content: cognitive_prompt,
                    thinking_content: None,
                    model: "system".to_string(),
                    timestamp: 0,
                    message_type: MessageType::Say,
                });
            }
        }

        // å±‚4: æ·»åŠ æœ€è¿‘çš„å¯¹è¯æ¶ˆæ¯ï¼ŒåŠ¨æ€è°ƒæ•´æ•°é‡ä»¥é€‚åº”ä¸Šä¸‹æ–‡çª—å£
        // é¢„ç•™ system æ¶ˆæ¯ + style hint + è¾“å‡º token çš„ç©ºé—´
        // ä¿å®ˆä¼°è®¡ï¼šè¾“å‡ºé¢„ç•™ 4096 tokenï¼Œstyle hint çº¦ 200 token
        let max_context_tokens: usize = 120_000;
        let reserved_tokens = system_token_budget + 4096 + 200;
        let available_for_history = if max_context_tokens > reserved_tokens {
            max_context_tokens - reserved_tokens
        } else {
            8000 // æœ€å°‘ä¿ç•™ 8000 token ç»™å†å²æ¶ˆæ¯
        };

        // ä»æœ€æ–°æ¶ˆæ¯å¼€å§‹å‘å‰ç´¯ç§¯ï¼Œç›´åˆ°è¾¾åˆ° token é¢„ç®—
        let mut selected_messages: Vec<Message> = Vec::new();
        let mut accumulated_tokens: usize = 0;
        let max_messages = 20usize; // æœ€å¤šä¿ç•™ 20 æ¡

        for msg in non_system.iter().rev() {
            let msg_tokens = msg.content.len() / 2;
            if selected_messages.len() >= max_messages {
                break;
            }
            if accumulated_tokens + msg_tokens > available_for_history && !selected_messages.is_empty() {
                // å·²ç»æœ‰æ¶ˆæ¯äº†ï¼Œè¶…å‡ºé¢„ç®—å°±åœæ­¢
                break;
            }
            accumulated_tokens += msg_tokens;
            selected_messages.push((*msg).clone());
        }

        // åè½¬å›æ—¶é—´é¡ºåº
        selected_messages.reverse();
        enhanced_messages.extend(selected_messages);

        // å±‚5: é£æ ¼çº¦æŸï¼ˆsay/do æ¨¡å¼æç¤ºï¼‰â€” ç”±è°ƒç”¨æ–¹åœ¨å¤–éƒ¨æ³¨å…¥
        // å±‚5.5: å›å¤å¤šæ ·æ€§çº¦æŸï¼ˆé˜²æ­¢ AI å›å¤æ¨¡å¼å›ºåŒ–ï¼‰
        let diversity_hint = Self::build_diversity_hint(&non_system);
        if !diversity_hint.is_empty() {
            enhanced_messages.push(Message {
                id: String::new(),
                role: MessageRole::System,
                content: diversity_hint,
                thinking_content: None,
                model: "system".to_string(),
                timestamp: 0,
                message_type: MessageType::Say,
            });
        }

        enhanced_messages
    }

    /// åˆ†ææœ€è¿‘çš„ AI å›å¤æ¨¡å¼ï¼Œç”Ÿæˆå¤šæ ·æ€§çº¦æŸæç¤º
    /// é˜²æ­¢ AI é™·å…¥å›ºå®šçš„å›å¤æ¨¡æ¿ï¼ˆå¦‚æ¯æ¬¡éƒ½ç”¨ç›¸åŒå¥å¼å¼€å¤´ï¼‰
    fn build_diversity_hint(recent_messages: &[&Message]) -> String {
        let ai_messages: Vec<&&Message> = recent_messages
            .iter()
            .filter(|m| m.role == MessageRole::Assistant)
            .collect();

        if ai_messages.len() < 3 {
            return String::new();
        }

        // æ£€æµ‹æœ€è¿‘ AI å›å¤çš„å¼€å¤´æ¨¡å¼
        let recent_starts: Vec<String> = ai_messages
            .iter()
            .rev()
            .take(5)
            .map(|m| {
                m.content
                    .chars()
                    .take(10)
                    .collect::<String>()
            })
            .collect();

        // æ£€æµ‹é‡å¤å¼€å¤´
        let mut start_freq: std::collections::HashMap<String, usize> = std::collections::HashMap::new();
        for start in &recent_starts {
            let key = start.chars().take(4).collect::<String>();
            *start_freq.entry(key).or_insert(0) += 1;
        }

        let has_repetitive_starts = start_freq.values().any(|&count| count >= 3);

        // æ£€æµ‹å›å¤é•¿åº¦çš„æ–¹å·®ï¼ˆå¦‚æœæ–¹å·®å¤ªå°è¯´æ˜é•¿åº¦å¤ªå›ºå®šï¼‰
        let lengths: Vec<f64> = ai_messages
            .iter()
            .rev()
            .take(5)
            .map(|m| m.content.chars().count() as f64)
            .collect();

        let mean_len = lengths.iter().sum::<f64>() / lengths.len() as f64;
        let variance = lengths.iter().map(|l| (l - mean_len).powi(2)).sum::<f64>() / lengths.len() as f64;
        let cv = if mean_len > 0.0 { variance.sqrt() / mean_len } else { 0.0 }; // å˜å¼‚ç³»æ•°

        let has_fixed_length = cv < 0.15 && lengths.len() >= 4; // å˜å¼‚ç³»æ•° < 15% è¯´æ˜é•¿åº¦å¤ªå›ºå®š

        if !has_repetitive_starts && !has_fixed_length {
            return String::new();
        }

        let mut hint = String::from("ã€å›å¤å¤šæ ·æ€§è¦æ±‚ã€‘\n");
        if has_repetitive_starts {
            hint.push_str("ä½ æœ€è¿‘çš„å›å¤å¼€å¤´å¤ªç›¸ä¼¼äº†ï¼Œæ¢ä¸€ç§å®Œå…¨ä¸åŒçš„æ–¹å¼å¼€å§‹ã€‚\n");
            hint.push_str("è¯•è¯•ï¼šç”¨åŠ¨ä½œå¼€å¤´ã€åé—®ã€æ„Ÿå¹ã€ç›´æ¥å›åº”å¯¹æ–¹æŸä¸ªè¯ã€æ²‰é»˜åçªç„¶è¯´ä¸€å¥ã€å‘ä¸ªè¡¨æƒ…å†è¯´è¯\n");
        }
        if has_fixed_length {
            hint.push_str(&format!(
                "ä½ æœ€è¿‘çš„å›å¤é•¿åº¦éƒ½åœ¨{}å­—å·¦å³ï¼Œå¤ªæœºæ¢°äº†ã€‚çœŸäººèŠå¤©é•¿çŸ­ä¸ä¸€ï¼š\n\
                 æœ‰æ—¶åªå›ä¸€ä¸ªã€Œå—¯ã€ï¼Œæœ‰æ—¶çªç„¶è¯´ä¸€å¤§æ®µã€‚æ ¹æ®æƒ…ç»ªå’Œæƒ…æ™¯è‡ªç„¶å˜åŒ–ã€‚\n",
                mean_len.round() as u32
            ));
        }
        hint
    }

    /// Send a message: validate â†’ detect type â†’ persist user msg â†’ build request â†’ get JWT â†’ stream SSE â†’ persist assistant msg â†’ check memory.
    pub async fn send_message(
        &self,
        conversation_id: &str,
        content: &str,
        model: &str,
        enable_thinking: bool,
        on_event: impl Fn(ChatStreamEvent),
    ) -> Result<(), ChatError> {
        Self::validate_message(content)?;

        // è‡ªåŠ¨æ£€æµ‹ say/do ç±»å‹
        let message_type = Self::detect_message_type(content);

        let user_msg = Message {
            id: uuid::Uuid::new_v4().to_string(),
            role: MessageRole::User,
            content: content.to_string(),
            thinking_content: None,
            model: model.to_string(),
            timestamp: chrono::Utc::now().timestamp_millis(),
            message_type: message_type.clone(),
        };
        self.conversation_store
            .add_message(conversation_id, user_msg)?;

        // å¢åŠ è½®æ¬¡è®¡æ•°
        self.conversation_store
            .increment_turn_count(conversation_id)?;

        let conv = self
            .conversation_store
            .load_conversation(conversation_id)?;

        // åŠ è½½è®°å¿†ç´¢å¼•
        let memory_summaries = self
            .memory_engine
            .load_memory_index(conversation_id)
            .unwrap_or_default();

        // æ„å»ºä¸Šä¸‹æ–‡å¢å¼ºçš„æ¶ˆæ¯åˆ—è¡¨
        let mut enhanced_messages =
            Self::build_context_enhanced_messages(&conv, content, &memory_summaries);

        // æ³¨å…¥ say/do æ¨¡å¼æç¤ºï¼ˆæ’å…¥åˆ°æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ä¹‹å‰ï¼Œç¡®ä¿ç”¨æˆ·æ¶ˆæ¯æ˜¯æœ€åä¸€æ¡ï¼‰
        let style_hint = SayDoDetector::build_style_prompt(&message_type);
        let style_msg = Message {
            id: String::new(),
            role: MessageRole::System,
            content: style_hint.to_string(),
            thinking_content: None,
            model: "system".to_string(),
            timestamp: 0,
            message_type: MessageType::Say,
        };
        // æ‰¾åˆ°æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯çš„ä½ç½®ï¼Œå°† style hint æ’å…¥åˆ°å®ƒä¹‹å‰
        let last_user_idx = enhanced_messages
            .iter()
            .rposition(|m| m.role == MessageRole::User);
        if let Some(idx) = last_user_idx {
            enhanced_messages.insert(idx, style_msg);
        } else {
            enhanced_messages.push(style_msg);
        }

        // è‡ªåŠ¨å†³å®šæ˜¯å¦å¯ç”¨æ€è€ƒ
        let actual_thinking = Self::should_enable_thinking(model, enable_thinking);

        let request_body = Self::build_request_body(&enhanced_messages, model, actual_thinking);

        let token = {
            let mut auth = self.jwt_auth.lock().unwrap();
            auth.get_token()
        };

        let (full_content, full_thinking) = StreamingHandler::stream_chat(
            BIGMODEL_API_URL,
            &token,
            request_body,
            &on_event,
        )
        .await?;

        // å¦‚æœ AI è¿”å›äº†ç©ºå†…å®¹ï¼Œå‘é€ Done äº‹ä»¶è®©å‰ç«¯æ­£ç¡®ç»“æŸæµå¼çŠ¶æ€
        if full_content.trim().is_empty() {
            if !full_thinking.is_empty() {
                on_event(ChatStreamEvent::Error(
                    "AI æ€è€ƒè¿‡ç¨‹æ¶ˆè€—äº†å…¨éƒ¨ token é¢„ç®—ï¼Œæœªèƒ½ç”Ÿæˆå›å¤å†…å®¹ã€‚è¯·é‡è¯•æˆ–å…³é—­æ€è€ƒæ¨¡å¼ã€‚".to_string(),
                ));
            }
            on_event(ChatStreamEvent::Done);
            return Ok(());
        }

        let thinking = if full_thinking.is_empty() {
            None
        } else {
            Some(full_thinking)
        };

        let assistant_msg = Message {
            id: uuid::Uuid::new_v4().to_string(),
            role: MessageRole::Assistant,
            content: full_content,
            thinking_content: thinking,
            model: model.to_string(),
            timestamp: chrono::Utc::now().timestamp_millis(),
            message_type: MessageType::Say,
        };
        self.conversation_store
            .add_message(conversation_id, assistant_msg)?;

        // Send Done after message is persisted so Flutter reloads the saved data
        on_event(ChatStreamEvent::Done);

        Ok(())
    }

    /// é‡æ–°ç”ŸæˆAIå›å¤ï¼šä¸æ·»åŠ ç”¨æˆ·æ¶ˆæ¯ï¼Œç›´æ¥åŸºäºç°æœ‰å¯¹è¯ä¸Šä¸‹æ–‡é‡æ–°è¯·æ±‚AI
    pub async fn regenerate_response(
        &self,
        conversation_id: &str,
        model: &str,
        enable_thinking: bool,
        on_event: impl Fn(ChatStreamEvent),
    ) -> Result<(), ChatError> {
        let conv = self
            .conversation_store
            .load_conversation(conversation_id)?;

        // æ‰¾åˆ°æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯çš„å†…å®¹ï¼ˆç”¨äºæ„å»ºä¸Šä¸‹æ–‡ï¼‰
        let last_user_content = conv
            .messages
            .iter()
            .rev()
            .find(|m| m.role == MessageRole::User)
            .map(|m| m.content.clone())
            .unwrap_or_default();

        if last_user_content.is_empty() {
            return Err(ChatError::ValidationError {
                message: "No user message found to regenerate from".to_string(),
            });
        }

        let message_type = Self::detect_message_type(&last_user_content);

        // åŠ è½½è®°å¿†ç´¢å¼•
        let memory_summaries = self
            .memory_engine
            .load_memory_index(conversation_id)
            .unwrap_or_default();

        // æ„å»ºä¸Šä¸‹æ–‡å¢å¼ºçš„æ¶ˆæ¯åˆ—è¡¨
        let mut enhanced_messages =
            Self::build_context_enhanced_messages(&conv, &last_user_content, &memory_summaries);

        // æ³¨å…¥ say/do æ¨¡å¼æç¤ºï¼ˆæ’å…¥åˆ°æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯ä¹‹å‰ï¼Œç¡®ä¿ç”¨æˆ·æ¶ˆæ¯æ˜¯æœ€åä¸€æ¡ï¼‰
        let style_hint = SayDoDetector::build_style_prompt(&message_type);
        let style_msg = Message {
            id: String::new(),
            role: MessageRole::System,
            content: style_hint.to_string(),
            thinking_content: None,
            model: "system".to_string(),
            timestamp: 0,
            message_type: MessageType::Say,
        };
        let last_user_idx = enhanced_messages
            .iter()
            .rposition(|m| m.role == MessageRole::User);
        if let Some(idx) = last_user_idx {
            enhanced_messages.insert(idx, style_msg);
        } else {
            enhanced_messages.push(style_msg);
        }

        let actual_thinking = Self::should_enable_thinking(model, enable_thinking);
        let request_body = Self::build_request_body(&enhanced_messages, model, actual_thinking);

        let token = {
            let mut auth = self.jwt_auth.lock().unwrap();
            auth.get_token()
        };

        let (full_content, full_thinking) = StreamingHandler::stream_chat(
            BIGMODEL_API_URL,
            &token,
            request_body,
            &on_event,
        )
        .await?;

        // å¦‚æœ AI è¿”å›äº†ç©ºå†…å®¹ï¼Œå‘é€ Done äº‹ä»¶è®©å‰ç«¯æ­£ç¡®ç»“æŸæµå¼çŠ¶æ€
        if full_content.trim().is_empty() {
            if !full_thinking.is_empty() {
                on_event(ChatStreamEvent::Error(
                    "AI æ€è€ƒè¿‡ç¨‹æ¶ˆè€—äº†å…¨éƒ¨ token é¢„ç®—ï¼Œæœªèƒ½ç”Ÿæˆå›å¤å†…å®¹ã€‚è¯·é‡è¯•æˆ–å…³é—­æ€è€ƒæ¨¡å¼ã€‚".to_string(),
                ));
            }
            on_event(ChatStreamEvent::Done);
            return Ok(());
        }

        let thinking = if full_thinking.is_empty() {
            None
        } else {
            Some(full_thinking)
        };

        let assistant_msg = Message {
            id: uuid::Uuid::new_v4().to_string(),
            role: MessageRole::Assistant,
            content: full_content,
            thinking_content: thinking,
            model: model.to_string(),
            timestamp: chrono::Utc::now().timestamp_millis(),
            message_type: MessageType::Say,
        };
        self.conversation_store
            .add_message(conversation_id, assistant_msg)?;

        // Send Done after message is persisted so Flutter reloads the saved data
        on_event(ChatStreamEvent::Done);

        Ok(())
    }

    /// æ‰§è¡Œè®°å¿†æ€»ç»“ï¼ˆç”±å¤–éƒ¨è°ƒç”¨ï¼Œåœ¨ send_message å®Œæˆåå¼‚æ­¥è§¦å‘ï¼‰
    /// é‡‡ç”¨åŒé˜¶æ®µéªŒè¯ï¼š
    ///   é˜¶æ®µ1: ä½¿ç”¨æ€»ç»“æ¨¡å‹ç”Ÿæˆæ‘˜è¦
    ///   é˜¶æ®µ2: ä½¿ç”¨éªŒè¯ prompt æ£€æŸ¥æ ¸å¿ƒäº‹å®å®Œæ•´æ€§ï¼ˆå½“å·²æœ‰æ‘˜è¦æ—¶ï¼‰
    pub async fn summarize_memory(
        &self,
        conversation_id: &str,
        on_event: impl Fn(ChatStreamEvent),
    ) -> Result<Option<MemorySummary>, ChatError> {
        let conv = self
            .conversation_store
            .load_conversation(conversation_id)?;

        if !MemoryEngine::should_summarize(conv.turn_count) {
            return Ok(None);
        }

        // è·å–éœ€è¦æ€»ç»“çš„æ¶ˆæ¯èŒƒå›´
        let turn_start = if conv.turn_count > 10 {
            conv.turn_count - 10 + 1
        } else {
            1
        };
        let turn_end = conv.turn_count;

        // è·å–æœ€è¿‘ 20 æ¡æ¶ˆæ¯ç”¨äºæ€»ç»“
        let recent_messages: Vec<Message> = conv
            .messages
            .iter()
            .filter(|m| m.role != MessageRole::System)
            .rev()
            .take(20)
            .cloned()
            .collect::<Vec<_>>()
            .into_iter()
            .rev()
            .collect();

        let existing_summaries = self
            .memory_engine
            .load_memory_index(conversation_id)
            .unwrap_or_default();

        // åŠ¨æ€é€‰æ‹©æ€»ç»“æ¨¡å‹
        let summary_model = Self::choose_summary_model(&conv.messages);

        // â”€â”€ é˜¶æ®µ1: ç”Ÿæˆæ‘˜è¦ â”€â”€
        // å½“å·²æœ‰å¤šæ®µæ‘˜è¦æ—¶ï¼Œä½¿ç”¨é•¿æ‘˜è¦æ•´åˆ promptï¼›å¦åˆ™ä½¿ç”¨æ ‡å‡† prompt
        let prompt = if existing_summaries.len() >= 3 {
            MemoryEngine::build_long_summary_prompt(&existing_summaries, &recent_messages)
        } else {
            MemoryEngine::build_summarize_prompt(
                &recent_messages,
                &existing_summaries,
                turn_start,
                turn_end,
            )
        };

        let summary_messages = vec![
            Message {
                id: String::new(),
                role: MessageRole::System,
                content: "ä½ æ˜¯ä¸€ä¸ªç²¾ç¡®çš„è®°å¿†ç®¡ç†ç³»ç»Ÿï¼Œè´Ÿè´£æ€»ç»“å¯¹è¯å†…å®¹ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§è¦æ±‚çš„JSONæ ¼å¼è¾“å‡ºã€‚".to_string(),
                thinking_content: None,
                model: "system".to_string(),
                timestamp: 0,
                message_type: MessageType::Say,
            },
            Message {
                id: String::new(),
                role: MessageRole::User,
                content: prompt,
                thinking_content: None,
                model: summary_model.to_string(),
                timestamp: 0,
                message_type: MessageType::Say,
            },
        ];

        // æ€»ç»“ä¸é™åˆ¶ max_tokensï¼ˆä¼  4096 ä»¥ç¡®ä¿å®Œæ•´è¾“å‡ºï¼‰
        let request_body = Self::build_request_body_with_options(&summary_messages, summary_model, false, Some(4096));

        let token = {
            let mut auth = self.jwt_auth.lock().unwrap();
            auth.get_token()
        };

        let (summary_text, _) = StreamingHandler::stream_chat(
            BIGMODEL_API_URL,
            &token,
            request_body,
            &on_event,
        )
        .await?;

        // è§£ææ€»ç»“ç»“æœ
        let parsed = match Self::parse_summary_json(&summary_text) {
            Ok(p) => p,
            Err(_) => return Ok(None),
        };

        let (final_summary, mut final_core_facts) = parsed;

        // â”€â”€ é˜¶æ®µ2: æ ¸å¿ƒäº‹å®å®Œæ•´æ€§éªŒè¯ï¼ˆå½“å·²æœ‰æ‘˜è¦æ—¶ï¼‰ â”€â”€
        if !existing_summaries.is_empty() {
            let original_facts: Vec<String> = existing_summaries
                .iter()
                .flat_map(|s| s.core_facts.clone())
                .collect();

            let verify_prompt = MemoryEngine::build_verify_summary_prompt(
                &original_facts,
                &final_summary,
                &final_core_facts,
            );

            let verify_messages = vec![
                Message {
                    id: String::new(),
                    role: MessageRole::System,
                    content: "ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„äº‹å®éªŒè¯ç³»ç»Ÿã€‚è¯·æ£€æŸ¥æ–°æ€»ç»“æ˜¯å¦å®Œæ•´ä¿ç•™äº†æ‰€æœ‰åŸå§‹æ ¸å¿ƒäº‹å®ã€‚åªè¾“å‡ºJSONã€‚".to_string(),
                    thinking_content: None,
                    model: "system".to_string(),
                    timestamp: 0,
                    message_type: MessageType::Say,
                },
                Message {
                    id: String::new(),
                    role: MessageRole::User,
                    content: verify_prompt,
                    thinking_content: None,
                    model: "glm-4.7-flash".to_string(),
                    timestamp: 0,
                    message_type: MessageType::Say,
                },
            ];

            // éªŒè¯ä½¿ç”¨ glm-4.7-flashï¼ˆå¿«é€Ÿä¸”è¶³å¤Ÿï¼‰
            let verify_body = Self::build_request_body_with_options(
                &verify_messages,
                "glm-4.7-flash",
                false,
                Some(2048),
            );

            let verify_token = {
                let mut auth = self.jwt_auth.lock().unwrap();
                auth.get_token()
            };

            // éªŒè¯é˜¶æ®µçš„äº‹ä»¶ä¸ä¼ é€’ç»™å‰ç«¯ï¼ˆé™é»˜æ‰§è¡Œï¼‰
            if let Ok((verify_text, _)) = StreamingHandler::stream_chat(
                BIGMODEL_API_URL,
                &verify_token,
                verify_body,
                |_| {}, // é™é»˜ï¼Œä¸å‘å‰ç«¯å‘é€éªŒè¯é˜¶æ®µçš„æµäº‹ä»¶
            )
            .await
            {
                // å°è¯•è§£æéªŒè¯ç»“æœ
                if let Some(start) = verify_text.find('{') {
                    if let Some(end) = verify_text.rfind('}') {
                        if let Ok(verify_json) =
                            serde_json::from_str::<serde_json::Value>(&verify_text[start..=end])
                        {
                            let is_valid = verify_json
                                .get("is_valid")
                                .and_then(|v| v.as_bool())
                                .unwrap_or(true);

                            if !is_valid {
                                // ä½¿ç”¨ä¿®æ­£åçš„æ ¸å¿ƒäº‹å®
                                if let Some(corrected) = verify_json
                                    .get("corrected_core_facts")
                                    .and_then(|v| v.as_array())
                                {
                                    let corrected_facts: Vec<String> = corrected
                                        .iter()
                                        .filter_map(|v| v.as_str().map(|s| s.to_string()))
                                        .collect();
                                    if !corrected_facts.is_empty() {
                                        final_core_facts = corrected_facts;
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }

        // æ„å»ºæœ€ç»ˆè®°å¿†æ‘˜è¦
        let keywords = MemoryEngine::extract_keywords(&final_summary);
        let mut all_keywords = keywords;
        for fact in &final_core_facts {
            all_keywords.extend(MemoryEngine::extract_keywords(fact));
        }
        all_keywords.sort();
        all_keywords.dedup();

        let memory = MemorySummary {
            id: uuid::Uuid::new_v4().to_string(),
            summary: final_summary,
            core_facts: final_core_facts,
            turn_range_start: turn_start,
            turn_range_end: turn_end,
            created_at: chrono::Utc::now().timestamp_millis(),
            keywords: all_keywords,
        };

        // ä¿å­˜åˆ°è®°å¿†ç´¢å¼•
        let mut summaries = existing_summaries;
        summaries.push(memory.clone());
        self.memory_engine
            .save_memory_index(conversation_id, &summaries)?;

        // åŒæ—¶æ›´æ–°å¯¹è¯ä¸­çš„è®°å¿†æ‘˜è¦
        self.conversation_store
            .update_memory_summaries(conversation_id, &summaries)?;

        Ok(Some(memory))
    }

    /// è§£ææ€»ç»“ JSON
    fn parse_summary_json(text: &str) -> Result<(String, Vec<String>), String> {
        // å°è¯•æå– JSON éƒ¨åˆ†
        let json_str = if let Some(start) = text.find('{') {
            if let Some(end) = text.rfind('}') {
                &text[start..=end]
            } else {
                text
            }
        } else {
            text
        };

        let json: serde_json::Value =
            serde_json::from_str(json_str).map_err(|e| format!("JSON parse error: {}", e))?;

        let summary = json
            .get("summary")
            .and_then(|v| v.as_str())
            .unwrap_or("")
            .to_string();

        let core_facts: Vec<String> = json
            .get("core_facts")
            .and_then(|v| v.as_array())
            .map(|arr| {
                arr.iter()
                    .filter_map(|v| v.as_str().map(|s| s.to_string()))
                    .collect()
            })
            .unwrap_or_default();

        Ok((summary, core_facts))
    }

    /// é‡å¯å‰§æƒ…ï¼šæ¸…é™¤å¯¹è¯æ¶ˆæ¯ä½†ä¿ç•™ system prompt å’Œè§’è‰²å¼€åœºç™½
    pub fn restart_story(
        &self,
        conversation_id: &str,
    ) -> Result<(), ChatError> {
        let mut conv = self
            .conversation_store
            .load_conversation(conversation_id)?;

        // ä¿ç•™ system æ¶ˆæ¯å’Œç¬¬ä¸€æ¡ assistant æ¶ˆæ¯ï¼ˆå¼€åœºç™½ï¼‰
        let mut kept_messages: Vec<Message> = Vec::new();
        let mut found_greeting = false;

        for msg in &conv.messages {
            if msg.role == MessageRole::System {
                kept_messages.push(msg.clone());
            } else if msg.role == MessageRole::Assistant && !found_greeting {
                // ä¿ç•™ç¬¬ä¸€æ¡ AI æ¶ˆæ¯ä½œä¸ºå¼€åœºç™½
                kept_messages.push(msg.clone());
                found_greeting = true;
            }
        }

        conv.messages = kept_messages;
        conv.turn_count = 0;
        conv.memory_summaries.clear();
        conv.updated_at = chrono::Utc::now().timestamp_millis();

        self.conversation_store.save_conversation(&conv)?;

        // æ¸…é™¤è®°å¿†ç´¢å¼•
        self.memory_engine.delete_memory_index(conversation_id)?;

        Ok(())
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    fn make_message(role: MessageRole, content: &str) -> Message {
        Message {
            id: uuid::Uuid::new_v4().to_string(),
            role,
            content: content.to_string(),
            thinking_content: None,
            model: "glm-4-flash".to_string(),
            timestamp: chrono::Utc::now().timestamp_millis(),
            message_type: MessageType::Say,
        }
    }

    #[test]
    fn test_validate_message_rejects_empty_string() {
        assert!(ChatEngine::validate_message("").is_err());
    }

    #[test]
    fn test_validate_message_rejects_spaces_only() {
        assert!(ChatEngine::validate_message("   ").is_err());
    }

    #[test]
    fn test_validate_message_rejects_tabs_and_newlines() {
        assert!(ChatEngine::validate_message("\t\n\r\n  ").is_err());
    }

    #[test]
    fn test_validate_message_accepts_normal_text() {
        assert!(ChatEngine::validate_message("Hello").is_ok());
    }

    #[test]
    fn test_validate_message_accepts_text_with_surrounding_whitespace() {
        assert!(ChatEngine::validate_message("  Hello  ").is_ok());
    }

    #[test]
    fn test_validate_message_returns_validation_error_type() {
        match ChatEngine::validate_message("") {
            Err(ChatError::ValidationError { .. }) => {}
            other => panic!("Expected ValidationError, got {:?}", other),
        }
    }

    #[test]
    fn test_build_request_body_always_has_stream_true() {
        let messages = vec![make_message(MessageRole::User, "hi")];
        let body = ChatEngine::build_request_body(&messages, "glm-4-flash", false);
        assert_eq!(body["stream"], serde_json::json!(true));
    }

    #[test]
    fn test_build_request_body_correct_model() {
        let messages = vec![make_message(MessageRole::User, "hi")];
        let body = ChatEngine::build_request_body(&messages, "glm-4-long", false);
        assert_eq!(body["model"], serde_json::json!("glm-4-long"));
    }

    #[test]
    fn test_build_request_body_messages_array_matches() {
        let messages = vec![
            make_message(MessageRole::User, "Hello"),
            make_message(MessageRole::Assistant, "Hi there"),
            make_message(MessageRole::User, "How are you?"),
        ];
        let body = ChatEngine::build_request_body(&messages, "glm-4-flash", false);
        let api_msgs = body["messages"].as_array().unwrap();
        assert_eq!(api_msgs.len(), 3);
        assert_eq!(api_msgs[0]["role"], "user");
        assert_eq!(api_msgs[0]["content"], "Hello");
        assert_eq!(api_msgs[1]["role"], "assistant");
        assert_eq!(api_msgs[1]["content"], "Hi there");
        assert_eq!(api_msgs[2]["role"], "user");
        assert_eq!(api_msgs[2]["content"], "How are you?");
    }

    #[test]
    fn test_build_request_body_system_role() {
        let messages = vec![make_message(MessageRole::System, "You are helpful")];
        let body = ChatEngine::build_request_body(&messages, "glm-4-flash", false);
        let api_msgs = body["messages"].as_array().unwrap();
        assert_eq!(api_msgs[0]["role"], "system");
    }

    #[test]
    fn test_build_request_body_empty_messages() {
        let body = ChatEngine::build_request_body(&[], "glm-4-flash", false);
        let api_msgs = body["messages"].as_array().unwrap();
        assert!(api_msgs.is_empty());
        assert_eq!(body["stream"], serde_json::json!(true));
    }

    #[test]
    fn test_build_request_body_thinking_enabled_for_glm4_air() {
        let messages = vec![make_message(MessageRole::User, "think hard")];
        let body = ChatEngine::build_request_body(&messages, "glm-4-air", true);
        assert_eq!(body["thinking"], serde_json::json!({"type": "enabled"}));
    }

    #[test]
    fn test_build_request_body_no_thinking_for_glm4_air_disabled() {
        let messages = vec![make_message(MessageRole::User, "hi")];
        let body = ChatEngine::build_request_body(&messages, "glm-4-air", false);
        assert_eq!(body["thinking"], serde_json::json!({"type": "disabled"}));
    }

    #[test]
    fn test_build_request_body_thinking_disabled_explicitly() {
        let messages = vec![make_message(MessageRole::User, "hi")];
        // glm-4.7 with thinking disabled should explicitly send disabled
        let body = ChatEngine::build_request_body(&messages, "glm-4.7", false);
        assert_eq!(body["thinking"], serde_json::json!({"type": "disabled"}));
        // glm-4.7-flash with thinking disabled
        let body = ChatEngine::build_request_body(&messages, "glm-4.7-flash", false);
        assert_eq!(body["thinking"], serde_json::json!({"type": "disabled"}));
    }

    #[test]
    fn test_build_request_body_thinking_enabled_for_glm4_7() {
        let messages = vec![make_message(MessageRole::User, "think hard")];
        let body = ChatEngine::build_request_body(&messages, "glm-4.7", true);
        assert_eq!(body["thinking"], serde_json::json!({"type": "enabled"}));
    }

    #[test]
    fn test_build_request_body_no_thinking_for_unknown_model() {
        let messages = vec![make_message(MessageRole::User, "hi")];
        for model in &["glm-4-flash", "glm-4-long"] {
            let body = ChatEngine::build_request_body(&messages, model, true);
            assert!(body.get("thinking").is_none(), "Model {} should not have thinking param", model);
        }
    }

    #[test]
    fn test_build_request_body_stream_true_with_all_models() {
        let messages = vec![make_message(MessageRole::User, "test")];
        for model in &["glm-4.7", "glm-4-flash", "glm-4-air", "glm-4-long"] {
            let body = ChatEngine::build_request_body(&messages, model, false);
            assert_eq!(body["stream"], serde_json::json!(true), "stream should be true for model {}", model);
        }
    }

    #[test]
    fn test_build_request_body_preserves_message_content_exactly() {
        let content = "Hello ä½ å¥½ ğŸŒ\nnewline\ttab";
        let messages = vec![make_message(MessageRole::User, content)];
        let body = ChatEngine::build_request_body(&messages, "glm-4-flash", false);
        assert_eq!(body["messages"][0]["content"], content);
    }

    #[test]
    fn test_detect_message_type() {
        assert_eq!(ChatEngine::detect_message_type("ä½ å¥½"), MessageType::Say);
        assert_eq!(ChatEngine::detect_message_type("*èµ°è¿‡å»*"), MessageType::Do);
        assert_eq!(
            ChatEngine::detect_message_type("*èµ°è¿‡å»* ä½ å¥½"),
            MessageType::Mixed
        );
    }

    #[test]
    fn test_should_enable_thinking() {
        assert!(ChatEngine::should_enable_thinking("glm-4-air", true));
        assert!(!ChatEngine::should_enable_thinking("glm-4-air", false));
        assert!(ChatEngine::should_enable_thinking("glm-4.7", true));
        assert!(!ChatEngine::should_enable_thinking("glm-4.7", false));
        assert!(!ChatEngine::should_enable_thinking("glm-4.7-flash", true));
        assert!(!ChatEngine::should_enable_thinking("glm-4-long", true));
    }

    #[test]
    fn test_parse_summary_json() {
        let json = r#"{"summary": "æµ‹è¯•æ€»ç»“", "core_facts": ["äº‹å®1", "äº‹å®2"]}"#;
        let result = ChatEngine::parse_summary_json(json).unwrap();
        assert_eq!(result.0, "æµ‹è¯•æ€»ç»“");
        assert_eq!(result.1, vec!["äº‹å®1", "äº‹å®2"]);
    }

    #[test]
    fn test_parse_summary_json_with_extra_text() {
        let text = r#"å¥½çš„ï¼Œä»¥ä¸‹æ˜¯æ€»ç»“ï¼š
{"summary": "æ¦‚æ‹¬å†…å®¹", "core_facts": ["èº«ä»½ä¿¡æ¯"]}
ä»¥ä¸Šå°±æ˜¯æ€»ç»“ã€‚"#;
        let result = ChatEngine::parse_summary_json(text).unwrap();
        assert_eq!(result.0, "æ¦‚æ‹¬å†…å®¹");
    }
}
